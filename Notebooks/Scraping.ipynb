{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Installing the required packages using pip\n",
        "%pip install selenium\n",
        "%pip install webdriver-manager\n",
        "%pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "QLRCi4tYo32C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import csv"
      ],
      "metadata": {
        "id": "2v8SlxfKo8ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxxIHdiOov8D"
      },
      "outputs": [],
      "source": [
        "# Function to setup Chrome WebDriver\n",
        "def setup_driver():\n",
        "    options = webdriver.ChromeOptions()\n",
        "    # Initialize Chrome WebDriver with options\n",
        "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "    return driver\n",
        "\n",
        "# Function to get reading links from a given URL\n",
        "def get_reading_links(driver, url):\n",
        "    driver.get(url)\n",
        "    time.sleep(2) # Wait for the page to load\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    # Extract reading links using CSS selector\n",
        "    reading_links = [a['href'] for a in soup.select('a.CoveoResultLink')]\n",
        "    return reading_links\n",
        "\n",
        "# Function to scrape content from each reading link\n",
        "def scrape_reading_content(driver, links):\n",
        "    readings = []\n",
        "    for link in links:\n",
        "        driver.get(link)\n",
        "        time.sleep(2) # Wait for the page to load\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        # Extract relevant information from the page\n",
        "        title = soup.find('h1').text.strip() if soup.find('h1') else 'Title Not Found'\n",
        "\n",
        "        # Function to extract text based on header texts\n",
        "        def extract_text_by_header(soup, header_texts):\n",
        "            content = \"\"\n",
        "            # Normalize header_texts to lowercase for case-insensitive comparison\n",
        "            header_texts = [text.lower() for text in header_texts]\n",
        "            headers = soup.find_all('h2', class_='article-section')\n",
        "            for header in headers:\n",
        "                if header.text.strip().lower() in header_texts:\n",
        "                    current_element = header.find_next_sibling()\n",
        "                    while current_element and current_element.name != 'h2':\n",
        "                        if current_element.name in ['p', 'div', 'ol']:\n",
        "                            content += '\\n' + ' '.join(current_element.stripped_strings)\n",
        "                        current_element = current_element.find_next_sibling()\n",
        "                    if content:  # Break if content has been found\n",
        "                        break\n",
        "            return content.strip()\n",
        "\n",
        "        # Function to extract learning outcomes\n",
        "        def extract_learning_outcomes(soup):\n",
        "            content = \"\"\n",
        "            header = soup.find('h2', text=lambda t: \"Learning Outcomes\" in t, class_='article-section')\n",
        "            if header:\n",
        "                section = header.find_next_sibling('section')\n",
        "                if section:\n",
        "                    content = ' '.join(section.stripped_strings)\n",
        "            return content.strip()\n",
        "\n",
        "\n",
        "        # Extracts the introduction section from the page using predefined headers.\n",
        "        introduction = extract_text_by_header(soup, ['Introduction', 'Overview', 'INTRODUCTION'])\n",
        "        # Extracts the learning outcomes section from the page.\n",
        "        learning_outcomes = extract_learning_outcomes(soup)\n",
        "        # Extracts the summary section from the page using the 'Summary' header.\n",
        "        summary = extract_text_by_header(soup, ['Summary'])\n",
        "\n",
        "        # Find and extract the required publication year, level, links to the pdf\n",
        "        year = soup.find(\"span\", class_=\"content-utility-curriculum\").text.strip() if soup.find(\"span\", class_=\"content-utility-curriculum\") else \"Year Not Found\"\n",
        "        level = soup.find(\"span\", class_=\"content-utility-level\").text.strip() if soup.find(\"span\", class_=\"content-utility-level\") else \"Level Not Found\"\n",
        "        link_to_full_pdf = soup.find(\"a\", class_=\"locked-content\")[\"href\"].strip() if soup.find(\"a\", class_=\"locked-content\") else \"Link Not Found\"\n",
        "\n",
        "\n",
        "        # Appends the extracted information as a dictionary to the readings list.\n",
        "        readings.append({\n",
        "            'Name of the topic': title,\n",
        "            'Year': year,\n",
        "            'Level' : level,\n",
        "            'Introduction': introduction,\n",
        "            'Learning Outcomes': learning_outcomes,\n",
        "            'Summary': summary,\n",
        "            'Link to the Summary Page': link,\n",
        "            'Link to the PDF File': link_to_full_pdf\n",
        "        })\n",
        "\n",
        "    return readings\n",
        "\n",
        "# Function to save the scraped data into a CSV file.\n",
        "def save_to_csv(readings, filename=\"Team05.csv\"):\n",
        "    keys = readings[0].keys() # Extracts the keys from the first dictionary to use as column headers.\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
        "        dict_writer = csv.DictWriter(output_file, keys)\n",
        "        dict_writer.writeheader() # Writes the column headers.\n",
        "        dict_writer.writerows(readings) # Writes the rows of data.\n",
        "\n",
        "\n",
        "# Imports for handling specific exceptions from Selenium.\n",
        "from selenium.common.exceptions import NoSuchElementException, JavascriptException\n",
        "\n",
        "def setup_driver():\n",
        "    options = webdriver.ChromeOptions()\n",
        "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "    return driver\n",
        "\n",
        "def main():\n",
        "    driver = setup_driver() # Initializes the WebDriver.\n",
        "    # List of URLs to scrape.\n",
        "    urls = ['https://www.cfainstitute.org/en/membership/professional-development/refresher-readings#sort=%40refreadingcurriculumyear%20descending&numberOfResults=100',\n",
        "            'https://www.cfainstitute.org/en/membership/professional-development/refresher-readings#first=100&sort=%40refreadingcurriculumyear%20descending&numberOfResults=100',\n",
        "            'https://www.cfainstitute.org/en/membership/professional-development/refresher-readings#first=200&sort=%40refreadingcurriculumyear%20descending&numberOfResults=100',]\n",
        "    all_readings = []\n",
        "\n",
        "    try:\n",
        "        for url in urls:\n",
        "            links = get_reading_links(driver, url)\n",
        "            readings = scrape_reading_content(driver, links)\n",
        "            all_readings.extend(readings)\n",
        "        save_to_csv(all_readings)\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        ""
      ]
    }
  ]
}